# blog_automation.py
import requests
from bs4 import BeautifulSoup
import json
import re
import time

# ìˆ˜ì§‘í•  ë¸”ë¡œê·¸ ì£¼ì†Œ (2ê°œ ë¸”ë¡œê·¸)
BLOG_URLS = [
    "https://blog.naver.com/best-marking",
    "https://blog.naver.com/ijehkorea"  # ë§ˆì¼ì´ë§ˆì§€ êµ­ë‚´ ê³µì‹1í˜¸ ëŒ€ë¦¬ì  ì•„ì´ì œì½”ë¦¬ì•„
]

# ì œì™¸í•  í‚¤ì›Œë“œ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
EXCLUDE_KEYWORDS = [
    "ê³µì‹ëŒ€ë¦¬ì ", "ê³µì‹ ëŒ€ë¦¬ì ", "ëŒ€ë¦¬ì ", "ëŒ€ë¦¬ì ì—…ì²´", 
    "ì´ëŒ€ë¦¬ì ", "ê³µì‹ì´ëŒ€ë¦¬ì ", "ì •ì‹ëŒ€ë¦¬ì ", "ë…ì ëŒ€ë¦¬ì ",
    "ì˜ì—…ëŒ€í–‰", "íŒë§¤ëŒ€í–‰", "íŒë§¤ì²˜", "ì·¨ê¸‰ì "
]

def should_exclude_post(title, content):
    """
    ì œëª©ì´ë‚˜ ë‚´ìš©ì— ì œì™¸ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    """
    text_to_check = (title + " " + content).lower()
    
    for keyword in EXCLUDE_KEYWORDS:
        if keyword.lower() in text_to_check:
            print(f"  â†’ ì œì™¸ë¨: '{keyword}' í‚¤ì›Œë“œ í¬í•¨")
            return True
    return False

def get_blog_links(blog_url, max_posts=5):
    """
    ë¸”ë¡œê·¸ ë©”ì¸ì—ì„œ ìµœì‹  ê¸€ ë§í¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        blog_id = blog_url.split("/")[-1]
        base = f"https://blog.naver.com/PostList.naver?blogId={blog_id}"
        resp = requests.get(base, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(resp.text, "html.parser")
        
        scripts = soup.find_all("script")
        post_links = []
        
        for script in scripts:
            if "logNo=" in script.text:
                found = re.findall(r"logNo=(\d+)", script.text)
                for log in found:
                    link = f"https://blog.naver.com/{blog_id}/{log}"
                    if link not in post_links:
                        post_links.append(link)
            if len(post_links) >= max_posts * 2:  # ì—¬ìœ ë¶„ í™•ë³´
                break
        
        print(f"  ë¸”ë¡œê·¸ì—ì„œ {len(post_links)}ê°œ ë§í¬ ë°œê²¬")
        return post_links[:max_posts * 2]
        
    except Exception as e:
        print(f"  âŒ ë¸”ë¡œê·¸ ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def extract_post_content(post_url):
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê¸€ ë³¸ë¬¸ ì¶”ì¶œ
    """
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        res = requests.get(post_url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # iframe ì£¼ì†Œ ì¶”ì¶œ
        iframe = soup.find("iframe", {"id": "mainFrame"})
        if iframe:
            iframe_url = "https://blog.naver.com" + iframe["src"]
            res = requests.get(iframe_url, headers=headers)
            soup = BeautifulSoup(res.text, 'html.parser')
        
        # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
        content = ""
        
        # ë°©ë²• 1: se-main-container
        content_div = soup.find("div", {"class": "se-main-container"})
        if content_div:
            content = content_div.get_text(separator="\n").strip()
        
        # ë°©ë²• 2: postViewArea
        if not content:
            content_div = soup.find("div", id=re.compile("postViewArea"))
            if content_div:
                content = content_div.get_text(separator="\n").strip()
        
        # ë°©ë²• 3: se-component ëª¨ë“  ìš”ì†Œ
        if not content:
            components = soup.find_all("div", class_=re.compile("se-component"))
            content = "\n".join([comp.get_text(strip=True) for comp in components if comp.get_text(strip=True)])
        
        # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
        title = ""
        
        # ë°©ë²• 1: se_textarea
        title_tag = soup.find("h3", {"class": "se_textarea"})
        if title_tag:
            title = title_tag.get_text(strip=True)
        
        # ë°©ë²• 2: pcol1
        if not title:
            title_tag = soup.find("div", class_="pcol1")
            if title_tag:
                title = title_tag.get_text(strip=True)
        
        # ë°©ë²• 3: title íƒœê·¸
        if not title:
            title_tag = soup.find("title")
            if title_tag:
                title = title_tag.get_text(strip=True).replace(" : ë„¤ì´ë²„ ë¸”ë¡œê·¸", "")
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        if not title:
            title = "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"
        if not content:
            content = "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"
        
        return {
            "title": title,
            "link": post_url,
            "content": content[:1000]  # ì²˜ìŒ 1000ìë§Œ ì €ì¥
        }
        
    except Exception as e:
        print(f"    âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return {
            "title": "ì¶”ì¶œ ì‹¤íŒ¨",
            "link": post_url,
            "content": f"ì˜¤ë¥˜: {str(e)}"
        }

def main():
    collected = []
    excluded_count = 0
    
    print("=== ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìë™ ìˆ˜ì§‘ ì‹œì‘ ===")
    
    for i, blog_url in enumerate(BLOG_URLS, 1):
        print(f"\n[{i}/{len(BLOG_URLS)}] ë¸”ë¡œê·¸ ìˆ˜ì§‘: {blog_url}")
        
        # ë¸”ë¡œê·¸ ë§í¬ ìˆ˜ì§‘
        links = get_blog_links(blog_url, max_posts=8)  # ì—¬ìœ ë¶„ í™•ë³´
        
        valid_posts = 0
        for j, link in enumerate(links, 1):
            if valid_posts >= 5:  # ë¸”ë¡œê·¸ë‹¹ ìµœëŒ€ 5ê°œ
                break
                
            print(f"  [{j}] ìˆ˜ì§‘ ì¤‘: {link}")
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            post = extract_post_content(link)
            
            # ì œì™¸ í‚¤ì›Œë“œ ê²€ì‚¬
            if should_exclude_post(post["title"], post["content"]):
                excluded_count += 1
                continue
            
            # ìˆ˜ì§‘ ì„±ê³µ
            collected.append(post)
            valid_posts += 1
            print(f"    âœ… ìˆ˜ì§‘ ì™„ë£Œ: {post['title'][:50]}...")
            
            # ë„¤ì´ë²„ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
            time.sleep(3)
    
    # ê²°ê³¼ ì €ì¥
    with open("collected_posts.json", "w", encoding="utf-8") as f:
        json.dump(collected, f, ensure_ascii=False, indent=2)
    
    print(f"\n=== ìˆ˜ì§‘ ì™„ë£Œ ===")
    print(f"âœ… ì´ ìˆ˜ì§‘ëœ ê¸€: {len(collected)}ê°œ")
    print(f"âŒ ì œì™¸ëœ ê¸€: {excluded_count}ê°œ")
    print(f"ğŸ“ ì €ì¥ íŒŒì¼: collected_posts.json")
    
    # ìˆ˜ì§‘ëœ ê¸€ ì œëª© ë¯¸ë¦¬ë³´ê¸°
    if collected:
        print(f"\n=== ìˆ˜ì§‘ëœ ê¸€ ì œëª© ===")
        for i, post in enumerate(collected, 1):
            print(f"{i}. {post['title']}")

if __name__ == "__main__":
    main()
