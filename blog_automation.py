# blog_automation.py
import requests
from bs4 import BeautifulSoup
import json
import re
import time

# ìˆ˜ì§‘í•  ë¸”ë¡œê·¸ ì£¼ì†Œ (2ê°œ ë¸”ë¡œê·¸)
BLOG_URLS = [
    "https://blog.naver.com/best-marking",
    "https://blog.naver.com/ijehkorea"  # ë§ˆì¼ì´ë§ˆì§€ êµ­ë‚´ ê³µì‹1í˜¸ ëŒ€ë¦¬ì  ì•„ì´ì œì½”ë¦¬ì•„
]

# ì œê±°í•  í‚¤ì›Œë“œ (ë‚´ìš©ì—ì„œ ì‚­ì œí•  ë‹¨ì–´ë“¤)
REMOVE_KEYWORDS = [
    "ê³µì‹ëŒ€ë¦¬ì ", "ê³µì‹ ëŒ€ë¦¬ì ", "ëŒ€ë¦¬ì ", "ëŒ€ë¦¬ì ì—…ì²´", 
    "ì´ëŒ€ë¦¬ì ", "ê³µì‹ì´ëŒ€ë¦¬ì ", "ì •ì‹ëŒ€ë¦¬ì ", "ë…ì ëŒ€ë¦¬ì ",
    "ì˜ì—…ëŒ€í–‰", "íŒë§¤ëŒ€í–‰", "íŒë§¤ì²˜", "ì·¨ê¸‰ì "
]

def clean_text(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    """
    cleaned_text = text
    removed_words = []
    
    for keyword in REMOVE_KEYWORDS:
        if keyword in cleaned_text:
            cleaned_text = cleaned_text.replace(keyword, "")
            removed_words.append(keyword)
    
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    
    if removed_words:
        print(f"    ğŸ§¹ ì œê±°ëœ ë‹¨ì–´: {', '.join(removed_words)}")
    
    return cleaned_text

def get_blog_links(blog_url, max_posts=15):
    """
    ë¸”ë¡œê·¸ ë©”ì¸ì—ì„œ ìµœì‹  ê¸€ ë§í¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        blog_id = blog_url.split("/")[-1]
        base = f"https://blog.naver.com/PostList.naver?blogId={blog_id}"
        resp = requests.get(base, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(resp.text, "html.parser")
        
        scripts = soup.find_all("script")
        post_links = []
        
        for script in scripts:
            if "logNo=" in script.text:
                found = re.findall(r"logNo=(\d+)", script.text)
                for log in found:
                    link = f"https://blog.naver.com/{blog_id}/{log}"
                    if link not in post_links:
                        post_links.append(link)
            if len(post_links) >= max_posts * 2:  # ì—¬ìœ ë¶„ í™•ë³´
                break
        
        print(f"  ë¸”ë¡œê·¸ì—ì„œ {len(post_links)}ê°œ ë§í¬ ë°œê²¬")
        return post_links[:max_posts * 2]
        
    except Exception as e:
        print(f"  âŒ ë¸”ë¡œê·¸ ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def extract_post_content(post_url):
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê¸€ ë³¸ë¬¸ ì¶”ì¶œ
    """
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        res = requests.get(post_url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # iframe ì£¼ì†Œ ì¶”ì¶œ
        iframe = soup.find("iframe", {"id": "mainFrame"})
        if iframe:
            iframe_url = "https://blog.naver.com" + iframe["src"]
            res = requests.get(iframe_url, headers=headers)
            soup = BeautifulSoup(res.text, 'html.parser')
        
        # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
        content = ""
        
        # ë°©ë²• 1: se-main-container
        content_div = soup.find("div", {"class": "se-main-container"})
        if content_div:
            content = content_div.get_text(separator="\n").strip()
        
        # ë°©ë²• 2: postViewArea
        if not content:
            content_div = soup.find("div", id=re.compile("postViewArea"))
            if content_div:
                content = content_div.get_text(separator="\n").strip()
        
        # ë°©ë²• 3: se-component ëª¨ë“  ìš”ì†Œ
        if not content:
            components = soup.find_all("div", class_=re.compile("se-component"))
            content = "\n".join([comp.get_text(strip=True) for comp in components if comp.get_text(strip=True)])
        
        # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
        title = ""
        
        # ë°©ë²• 1: se_textarea
        title_tag = soup.find("h3", {"class": "se_textarea"})
        if title_tag:
            title = title_tag.get_text(strip=True)
        
        # ë°©ë²• 2: pcol1
        if not title:
            title_tag = soup.find("div", class_="pcol1")
            if title_tag:
                title = title_tag.get_text(strip=True)
        
        # ë°©ë²• 3: title íƒœê·¸
        if not title:
            title_tag = soup.find("title")
            if title_tag:
                title = title_tag.get_text(strip=True).replace(" : ë„¤ì´ë²„ ë¸”ë¡œê·¸", "")
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        if not title:
            title = "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"
        if not content:
            content = "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"
        
        # í‚¤ì›Œë“œ ì œê±° (ì œëª©ê³¼ ë‚´ìš©ì—ì„œ)
        cleaned_title = clean_text(title)
        cleaned_content = clean_text(content[:1000])  # ì²˜ìŒ 1000ìë§Œ ì €ì¥
        
        return {
            "title": cleaned_title,
            "link": post_url,
            "content": cleaned_content
        }
        
    except Exception as e:
        print(f"    âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return {
            "title": "ì¶”ì¶œ ì‹¤íŒ¨",
            "link": post_url,
            "content": f"ì˜¤ë¥˜: {str(e)}"
        }

def main():
    collected = []
    
    print("=== ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìë™ ìˆ˜ì§‘ ì‹œì‘ ===")
    
    for i, blog_url in enumerate(BLOG_URLS, 1):
        print(f"\n[{i}/{len(BLOG_URLS)}] ë¸”ë¡œê·¸ ìˆ˜ì§‘: {blog_url}")
        
        # ë¸”ë¡œê·¸ ë§í¬ ìˆ˜ì§‘ (15ê°œì”© ìˆ˜ì§‘)
        links = get_blog_links(blog_url, max_posts=15)
        
        valid_posts = 0
        for j, link in enumerate(links, 1):
            if valid_posts >= 15:  # ë¸”ë¡œê·¸ë‹¹ ìµœëŒ€ 15ê°œ
                break
                
            print(f"  [{j}] ìˆ˜ì§‘ ì¤‘: {link}")
            
            # ë³¸ë¬¸ ì¶”ì¶œ (í‚¤ì›Œë“œëŠ” ìë™ìœ¼ë¡œ ì œê±°ë¨)
            post = extract_post_content(link)
            
            # ìˆ˜ì§‘ ì„±ê³µ
            collected.append(post)
            valid_posts += 1
            print(f"    âœ… ìˆ˜ì§‘ ì™„ë£Œ: {post['title'][:50]}...")
            
            # ë„¤ì´ë²„ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸° (ëŒ€ëŸ‰ ìˆ˜ì§‘ ì‹œ ì•½ê°„ ëŠ˜ë¦¼)
            time.sleep(2)
    
    # ê²°ê³¼ ì €ì¥
    with open("collected_posts.json", "w", encoding="utf-8") as f:
        json.dump(collected, f, ensure_ascii=False, indent=2)
    
    print(f"\n=== ìˆ˜ì§‘ ì™„ë£Œ ===")
    print(f"âœ… ì´ ìˆ˜ì§‘ëœ ê¸€: {len(collected)}ê°œ")
    print(f"ğŸ“ ì €ì¥ íŒŒì¼: collected_posts.json")
    print(f"ğŸ§¹ ëª¨ë“  ê¸€ì—ì„œ ëŒ€ë¦¬ì  ê´€ë ¨ ë‹¨ì–´ê°€ ìë™ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ìˆ˜ì§‘ëœ ê¸€ ì œëª© ë¯¸ë¦¬ë³´ê¸°
    if collected:
        print(f"\n=== ìˆ˜ì§‘ëœ ê¸€ ì œëª© ===")
        for i, post in enumerate(collected, 1):
            print(f"{i}. {post['title']}")

if __name__ == "__main__":
    main()
