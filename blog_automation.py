import requests
from bs4 import BeautifulSoup
import json
import re
import time
import random

# ìˆ˜ì§‘í•  ë¸”ë¡œê·¸ ì£¼ì†Œ
BLOG_URLS = [
    "https://blog.naver.com/best-marking",
    "https://blog.naver.com/ijehkorea"
]

# ì œê±°í•  í‚¤ì›Œë“œ
REMOVE_KEYWORDS = [
    "ê³µì‹ëŒ€ë¦¬ì ", "ê³µì‹ ëŒ€ë¦¬ì ", "ëŒ€ë¦¬ì ", "ëŒ€ë¦¬ì ì—…ì²´",
    "ì´ëŒ€ë¦¬ì ", "ê³µì‹ì´ëŒ€ë¦¬ì ", "ì •ì‹ëŒ€ë¦¬ì ", "ë…ì ëŒ€ë¦¬ì ",
    "ì˜ì—…ëŒ€í–‰", "íŒë§¤ëŒ€í–‰", "íŒë§¤ì²˜", "ì·¨ê¸‰ì "
]

def clean_text(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    """
    cleaned_text = text
    for keyword in REMOVE_KEYWORDS:
        cleaned_text = cleaned_text.replace(keyword, "")
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    return cleaned_text

def get_blog_links(blog_url, max_posts=15):
    """
    ë¸”ë¡œê·¸ ë©”ì¸ì—ì„œ ìµœì‹  ê¸€ ë§í¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        blog_id = blog_url.split("/")[-1]
        base = f"https://blog.naver.com/PostList.naver?blogId={blog_id}"
        resp = requests.get(base, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(resp.text, "html.parser")

        scripts = soup.find_all("script")
        post_links = []

        for script in scripts:
            if "logNo=" in script.text:
                found = re.findall(r"logNo=(\d+)", script.text)
                for log in found:
                    link = f"https://blog.naver.com/{blog_id}/{log}"
                    if link not in post_links:
                        post_links.append(link)
            if len(post_links) >= max_posts * 2:
                break

        print(f"  ë¸”ë¡œê·¸ì—ì„œ {len(post_links)}ê°œ ë§í¬ ë°œê²¬")
        return post_links[:max_posts]
    except Exception as e:
        print(f"  âŒ ë¸”ë¡œê·¸ ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def extract_post_content(post_url):
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê¸€ ë³¸ë¬¸ ì¶”ì¶œ
    """
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(post_url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')

        # iframe ì£¼ì†Œ ì¶”ì¶œ
        iframe = soup.find("iframe", {"id": "mainFrame"})
        if iframe:
            iframe_url = "https://blog.naver.com" + iframe["src"]
            res = requests.get(iframe_url, headers=headers)
            soup = BeautifulSoup(res.text, 'html.parser')

        # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        content = ""
        content_div = soup.find("div", {"class": "se-main-container"})
        if content_div:
            content = content_div.get_text(separator="\n").strip()
        else:
            content_div = soup.find("div", id=re.compile("postViewArea"))
            if content_div:
                content = content_div.get_text(separator="\n").strip()
            else:
                components = soup.find_all("div", class_=re.compile("se-component"))
                content = "\n".join([comp.get_text(strip=True) for comp in components if comp.get_text(strip=True)])

        # ì œëª© ì¶”ì¶œ
        title = ""
        title_tag = soup.find("h3", {"class": "se_textarea"})
        if title_tag:
            title = title_tag.get_text(strip=True)
        else:
            title_tag = soup.find("div", class_="pcol1")
            if title_tag:
                title = title_tag.get_text(strip=True)
            else:
                title_tag = soup.find("title")
                if title_tag:
                    title = title_tag.get_text(strip=True).replace(" : ë„¤ì´ë²„ ë¸”ë¡œê·¸", "")

        # ê¸°ë³¸ê°’ ì„¤ì •
        if not title:
            title = "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"
        if not content:
            content = "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

        # í‚¤ì›Œë“œ ì œê±°
        cleaned_title = clean_text(title)
        cleaned_content = clean_text(content)

        return {
            "title": cleaned_title,
            "link": post_url,
            "content": cleaned_content
        }

    except Exception as e:
        print(f"    âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return {
            "title": "ì¶”ì¶œ ì‹¤íŒ¨",
            "link": post_url,
            "content": f"ì˜¤ë¥˜: {str(e)}"
        }

def main():
    collected = []

    print("=== ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìë™ ìˆ˜ì§‘ ì‹œì‘ ===")

    for i, blog_url in enumerate(BLOG_URLS, 1):
        print(f"\n[{i}/{len(BLOG_URLS)}] ë¸”ë¡œê·¸ ìˆ˜ì§‘: {blog_url}")

        links = get_blog_links(blog_url, max_posts=15)

        valid_posts = 0
        for j, link in enumerate(links, 1):
            if valid_posts >= 15:
                break

            print(f"  [{j}] ìˆ˜ì§‘ ì¤‘: {link}")

            post = extract_post_content(link)

            collected.append(post)
            valid_posts += 1
            print(f"    âœ… ìˆ˜ì§‘ ì™„ë£Œ: {post['title'][:50]}...")

            # ë„¤ì´ë²„ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ëŒ€ê¸°
            time.sleep(random.uniform(3, 5))

    # ê²°ê³¼ ì €ì¥
    with open("collected_posts.json", "w", encoding="utf-8") as f:
        json.dump(collected, f, ensure_ascii=False, indent=2)

    print(f"\n=== ìˆ˜ì§‘ ì™„ë£Œ ===")
    print(f"âœ… ì´ ìˆ˜ì§‘ëœ ê¸€: {len(collected)}ê°œ")
    print(f"ğŸ“ ì €ì¥ íŒŒì¼: collected_posts.json")
    print(f"ğŸ§¹ ëª¨ë“  ê¸€ì—ì„œ ëŒ€ë¦¬ì  ê´€ë ¨ ë‹¨ì–´ê°€ ìë™ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ìˆ˜ì§‘ëœ ê¸€ ì œëª© ë¯¸ë¦¬ë³´ê¸°
    if collected:
        print(f"\n=== ìˆ˜ì§‘ëœ ê¸€ ì œëª© ===")
        for i, post in enumerate(collected, 1):
            print(f"{i}. {post['title']}")

if __name__ == "__main__":
    main()
